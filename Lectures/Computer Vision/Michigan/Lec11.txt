00:00
all right welcome back to lecture 11 and
00:02
today we're gonna continue our
00:03
discussion about all the little
00:05
nitty-gritty tips and tricks that you
00:06
need to train neural networks I
00:08
apologize I couldn't come up with a
00:09
better title for this lecture at last
00:11
lecture they just end up being kind of a
00:13
bit of potpourri of a lot of little
00:14
things that you need that I think you
00:15
need to know about training neural
00:16
networks but sort of had a hard time
00:18
putting them into a good theme or a good
00:20
title other than that so to kind of
00:22
recap last lecture and also this lecture
00:25
like I said it's been a bit of a
00:26
potpourri of all a lot of different
00:28
topics that you need to know about about
00:29
how to train their own networks so the
00:31
last time we talked we focused on some
00:33
of these sort of one-time setup choices
00:36
about the architecture and whatnot but
00:37
you need to make before you start
00:39
training so recall last time we talked
00:41
about activation functions and we had a
00:43
lot to say about the different
00:44
activation functions but at the end we
00:45
just decided to stick with rail ooh
00:49
we talked about data pre-processing and
00:51
we finally explained the mystery behind
00:52
those means subtraction lines that have
00:54
been appearing on your homework
00:55
assignments so far we talked about
00:57
weight initialization and then saw how
01:00
we can use this Xavier or kind
01:03
initialization rules to force our
01:05
activations to be initialized torso
01:08
activations to have good distributions
01:10
over many layers of a deep network and
01:12
this was kind of a trade-off between too
01:13
small where things would collapse to
01:15
zero or too large where things would
01:16
explode and then these these last two
01:19
points I think we went a little bit fast
01:21
last time in lecture so if anyone have
01:23
any had any lingering questions about
01:25
any of these points this would be your
01:26
your in to ask those but then remember
01:30
we also talked last time about data
01:32
augmentation which was this technique by
01:34
which we can artificially multiply the
01:36
size of our training set by performing
01:38
random transformations on our training
01:40
data before we feed it directly into the
01:42
network and again we saw that data
01:44
augmentation was a way that you can
01:45
inject priors about your own knowledge
01:47
about the structure of your data into
01:50
the training procedure of your neural
01:52
network and that you could imagine
01:54
inventing different types of data
01:55
augmentation for different types of
01:56
tasks then we also saw this very general
02:00
concept of regularization where so far
02:03
in the in your homework assignments
02:05
you've seen something like l2
02:06
regularization where we add an explicit
02:08
term and or explicit
02:10
additional term on to our loss function
02:12
that for example penalize --is the norm
02:13
of the weight matrix but last time we
02:15
saw this much more general class of
02:17
regularizer z-- that are commonly used
02:19
in neural networks whereby in the
02:21
forward pass we somehow inject some kind
02:22
of noise to mess up the processing of
02:25
the neural network in some way and then
02:27
in the back in during testing then we
02:29
somehow marginalize out or average out
02:31
that bit of noise and as examples of
02:34
this sort of paradigm of regularization
02:35
we talked about things like dropouts
02:38
fractional pooling drop connect
02:40
stochastic depth and these crazy ones
02:42
like cut out and mix up that are
02:44
actually used in practice quite a bit so
02:47
I realize we went a little bit fast
02:48
especially around regularization toward
02:50
the end so I just wanted to make sure
02:51
there were no lingering questions about
02:53
any of these topics before we move on to
02:55
new stuff ok very good
03:00
so then that was kind of the the stuff
03:03
that we talked about last time and today
03:04
we're gonna talk move on to some other
03:06
interesting topics about bits and pieces
03:08
about how you train neural networks in
03:10
practice in particular we're going to
03:12
talk about things that you need to worry
03:14
about during the process of training
03:15
your model and getting your model to
03:17
Train that's sort of setting learning
03:19
rate schedules and how to choose hyper
03:21
parameters I know this has been a very
03:22
frustrating procedure so for some of you
03:24
and then some additional points that you
03:26
might want to think about after you've
03:28
successfully trained your model those
03:30
are questions about maybe model ensemble
03:33
incurring and how to scale up your model
03:35
to train on maybe whole data center
03:37
levels of compute so the first of these
03:40
topics is learning rate schedules so I
03:44
think at this point you've we've seen
03:46
many different optimization algorithms
03:48
we've seen things like vanilla s GD H GD
03:50
+ momentum biodegrade RMS from atom and
03:53
all of these have some kind of hyper
03:55
parameter that's called a learning rate
03:57
and usually this learning right hyper
03:59
parameter is probably the most important
04:01
hyper parameter that you need to set for
04:03
most deep learning models and at this
04:05
point you've had the chance to use SGD
04:07
for a variety of different types of
04:08
models and hopefully you've started to
04:10
get some some intuition about what
04:12
happens when you set different values of
04:14
a learning rate with different
04:15
optimizers so here on the left is a
04:17
little bit of a cartoon picture of what
04:19
you can sometimes expect might happen
04:21
with
04:22
optimize with with optimization as you
04:24
set different types of learning rates so
04:27
here for example in yellow if you set
04:29
the learning rate too high then often
04:31
things will just explode immediately and
04:33
the loss will escape to infinity or
04:35
you'll get Nan's and it will very
04:36
quickly go very wrong very fast in
04:40
contrast if you set something like in
04:42
blue a very low learning rate then
04:45
you'll see that learning tends to
04:46
proceed very very slowly and this is
04:49
good because you make progress things
04:50
don't explode to infinity but it might
04:52
take a while to train for your loss
04:54
actually you drop to very low values and
04:57
in contrast something like in green
05:00
might be learning it which is high but
05:02
not so high that you explode to infinity
05:04
and there you see that in contrast to
05:06
the blue learning rate setting a higher
05:08
learning rate might actually converge to
05:10
a fat to a value faster but it might
05:12
actually not converge to as low of a
05:14
lost value and and what we kind of like
05:17
is something like the red curve here
05:19
which is some sort of ideal good
05:21
learning rate that you see it makes a
05:24
quick progress towards this areas of low
05:26
loss while also not exploding to
05:28
infinity and also actually training
05:31
reasonably quickly so obviously if we
05:35
can we would prefer to choose this red
05:37
learning rate but if that's not always
05:39
possible we have a sort of a question
05:42
here which is that if we can't find that
05:44
one perfect learning rate that's going
05:45
to work for us then what are we supposed
05:47
to do how are we supposed to trade-off
05:50
between these seemingly sub optimal
05:51
choices and this is a bit of a trick
05:54
question it turns out because we don't
05:56
actually have to make one choice for the
05:58
learning rate in fact it's very common
06:00
to somehow choose all of them and to
06:03
basically the idea the basic idea here
06:05
is to start with a relic with a
06:07
relatively high learning rate so that
06:10
looks maybe something like the green
06:11
curve and that will allow our loss our
06:14
optimization to make very quick progress
06:16
in the first iterations of training
06:17
towards these areas of low loss and then
06:20
over time after maybe after this green
06:23
curve is sort of plateauing then we want
06:25
to reduce the learning rate and continue
06:27
training with these lower learning rates
06:29
maybe like this blue learning rate and
06:30
this will hopefully let us get the best
06:32
of both worlds
06:33
that we can hopefully buy by starting
06:35
with a high learning rate and then
06:37
lowering it over time that we can
06:39
hopefully make a quick progress at the
06:40
beginning and also converge to very -
06:43
Val - very low loss values at the end of
06:46
training but this has been sort of vague
06:49
and not very specific so far I just said
06:51
we're gonna start with a high learning
06:52
rate and then end with a low learning
06:54
rate but then what actually concretely
06:56
is that going to look like well this
06:59
this process of so this process of
07:01
choosing different mechanisms of
07:03
starting of changing the learning rate
07:05
over the process of training these are
07:07
called learning rate schedules and
07:08
there's several different forms of
07:10
learning rate schedules that are
07:11
commonly in use when training deep
07:13
neural network models the perhaps the
07:16
most commonly used learning rate
07:17
schedule is the so called step schedule
07:19
so here what we're going to do is start
07:23
with what with some value of the
07:24
learning rate for example in and for
07:27
example the residual networks are very
07:28
famous for using this kind of step
07:30
learning rate schedule so with a step
07:32
learning rate schedule we'll begin
07:33
training with some relatively high
07:35
learning rate like ten to the minus one
07:37
for a residual networks and then at
07:39
certain chosen points during the
07:41
optimization process we will decay the
07:43
lurk we will just all of a sudden jump
07:45
to a brand new lower learning rate so if
07:48
a residual networks the the schedule
07:49
that typically use here is to start with
07:52
learning rate at zero point one and then
07:54
after 30 deep arcs of training all of a
07:56
sudden restart the alert of drop the
07:58
learning rate to zero point zero one and
08:00
continue training for another thirty
08:01
bucks and then again drop the learning
08:04
rate again after 60 bucks and again
08:05
after ninety pox
08:06
we're basically after every 30 parts of
08:09
training we're going to drop the overall
08:10
learning rate by a factor of ten and if
08:14
you look at a training curve that is
08:16
here shown here on the left for one of
08:19
these so called step learning rate decay
08:21
schedules you get this very
08:22
characteristic curve of the of the loss
08:25
as a function of time that you get when
08:27
using a step learning rate decay you can
08:29
see that in this first phase of training
08:32
during the first 30 pox when we're using
08:34
this relatively high learning rate then
08:36
we see we're making a very quick
08:37
progress where the lost soil starts from
08:39
this high initial value and they make
08:41
sort of quick exponential progress
08:42
towards lower loss values
08:45
but then after about 30 bucks you can
08:47
see that this this quick progress has
08:49
somehow steadied off and we're no longer
08:51
making very fast progress after these
08:53
first 30 parts of training and after
08:55
these 30 epochs this moment when we
08:57
decay the learning rate and drop it by a
08:59
factor of 10 we see sort of another
09:01
another a new exponential pattern begin
09:03
where once we drop the learning rate
09:04
again it sort of decays and then
09:06
plateaus and then when we need the cable
09:08
and learning rate again at 60 epochs
09:10
sort of decays quickly and then plateaus
09:11
again so this is a very characteristic
09:14
schedule that you'll a very
09:16
characteristic shape of learning curves
09:18
that you'll see when models are trained
09:20
using this so-called step learning rate
09:22
schedule now one problem with this step
09:26
learning rate schedule is that it
09:27
introduces a lot of new a lot of new
09:29
hyper predators into the training of our
09:31
model now not only do we need to choose
09:33
the regularization and the initial
09:35
learning rate like we did in all
09:36
previous models we also need to choose
09:38
at which iterations are we going to
09:40
decay the learning rate and what are the
09:42
new learning rates that we're going to
09:44
choose at the iterations where we decay
09:46
it and that actually gives us a lot of
09:48
choices so properly tuning one of these
09:51
step decay schedules can actually take a
09:53
fair amount of trial and error so what
09:56
people usually do in practice is sort of
09:58
look at these learning curves and let
10:00
things train with the high learning rate
10:01
for quite a long time and then they get
10:03
a sense at which point the model tends
10:05
to Plateau so if you read papers
10:07
sometimes they'll say that they use a
10:09
heuristic where they keep training until
10:11
the lost plateaus or until the
10:12
validation accuracy foot plateaus and
10:14
then they decay the learning rate well
10:16
that usually means that they're using
10:17
some kind of heuristic we chosen step
10:19
decay schedule but as you can imagine if
10:22
you're starting out at a new piranha new
10:24
problem and you don't have a lot of time
10:26
to experiment with lots of different
10:27
decay schedules then this step decay
10:30
schedule can actually be a little bit
10:31
tricky because it introduces so many new
10:33
things into the model that you need to
10:35
tune so it overcomes some of those
10:37
shortcomings of the step decay schedule
10:39
one there's another learning rate
10:41
schedule that has become sort of trendy
10:43
in the past couple of years which is the
10:45
the so-called cosine learning rate decay
10:47
schedule so here rather than choosing
10:50
particular points particular iterations
10:52
at which we're going to decay the
10:53
learning rate instead we want to we're
10:56
going to write down some formula ahead
10:58
of time
10:58
that tells us what will the learning
11:00
rate be at every epoch as a function of
11:03
the epoch number or the iteration number
11:05
and then we need then then we only need
11:07
to choose some kind of functional form
11:09
that is the shape of the curve along
11:11
which this learning rate will decay so
11:14
one of these that's become very popular
11:16
is this cosa is this a half wave cosign
11:18
learning rate schedule where you can see
11:21
that from the plot on the right where we
11:23
show the the learning rate as a function
11:25
of time we can see that it starts off as
11:27
some high value and then the shape at
11:29
which the learning rate decays is equal
11:31
to one half of a period of a cosine wave
11:34
and this and then what this means is
11:37
that we start out at some initial high
11:39
value of the learning rate and towards
11:42
the end of training our learning rate
11:43
will decay all the way to zero as this
11:45
wave of the cosine K is all the way to
11:47
zero and now this this cosine learning
11:51
rate schedule is very is has is very
11:54
appealing because it has many many fewer
11:56
hyper parameters than the step decay
11:59
schedule so in particular the cosine
12:02
learning rate decay schedule only has
12:04
two hyper parameters that we need to
12:05
choose one is the initial learning rate
12:08
on this alpha zero here on the equation
12:10
and the other is the number of epochs
12:12
that were going to use to train the
12:14
model which is this capital T but what's
12:17
particularly appealing about this cosine
12:19
light rate schedule is that it actually
12:20
doesn't introduce any new hyper
12:22
parameters when training the model
12:23
because whenever we're training we're
12:26
not on the model we always need to
12:27
choose some initial some learning rate
12:29
and we always need to choose some number
12:31
of iterations that we're going to train
12:32
so those two hyper parameters so when
12:36
when using cosine learning rate schedule
12:38
it doesn't introduce any new hyper
12:39
parameters
12:40
it just gives additional interpretation
12:42
or additional meaning to some of these
12:44
other Kuyper parameters that we already
12:45
were having to choose anyway before and
12:47
so that tends to make the cosine
12:49
schedules a lot more ease a bit easier
12:52
to tune compared to step decay schedules
12:54
and the general rule of thumb with
12:56
cosine schedules is just training longer
12:58
tends to work better so in practice the
13:00
only thing you really need to tune is
13:02
that initial learning rate and then sort
13:04
of come to grips with how long you're
13:05
willing to wait for your model to train
13:07
so that I think those are some reasons
13:09
why this hosiah learning wait just keep
13:11
the case schedule
13:12
has become reasonably popular in the
13:14
last couple of years and here I put some
13:16
citations on the slide of some
13:18
reasonably high profile papers from the
13:21
last year or two that have used this
13:22
cosine learning rate to keep schedule
13:25
but this cosine shape is just one of
13:28
many shapes that you might imagine using
13:30
for decaying learning rates over time
13:33
so another decay schedule that people
13:35
sometimes use is a simple linear decay
13:37
again we're going to start with some
13:38
initial learning rate and then decay it
13:40
to zero over the course of training but
13:42
rather than following this cosine decay
13:44
this gay decay learning schedule instead
13:46
will symbol simply decay the learning
13:48
rate linearly over time and that seems
13:51
to work well for many problems I should
13:53
point out that I think there has not
13:55
been super good studies that really
13:58
compare these different schedules
13:59
head-to-head so I can't really tell you
14:02
concretely when cosine is going to be
14:04
better or linear is going to work better
14:05
I think what most people do in practice
14:07
is they build upon some prior work and
14:09
then they sort of adopt whatever
14:11
whatever type of schedule happen to be
14:13
used in the prior work that they're
14:14
building upon so what this means is that
14:17
you'll see different areas of deep
14:18
learning tends to end up using different
14:21
types of learning rate schedules but
14:22
it's not really clear to me that that's
14:24
because they're intrinsically better for
14:25
that area it's often I think just
14:27
because they want to have a fair
14:28
comparison with whatever a piece of work
14:30
came beforehand so with that kind of in
14:33
mind if you kind of look at these
14:34
citations and what type of problem
14:36
you'll see that a lot of computer vision
14:38
type projects are often using this
14:40
cosine learning rate decay schedule
14:42
whereas this linear learning rate to
14:44
case schedule is often used for
14:45
large-scale natural language processing
14:46
instead that are also trained using deep
14:49
neural networks and again I think that's
14:51
maybe not something fundamental about
14:52
vision versus natural language I think
14:54
it's more a function of what paper what
14:55
what how different researchers in
14:57
different areas have proceeded upon the
14:59
paths so then another learning rate
15:01
schedule you'll sometimes see is this
15:04
inverse square root schedule that sort
15:07
of decays the learning rate across a
15:08
different functional form but again it
15:10
has this interpretation of starting out
15:12
high and then ending up low now this
15:16
inverse square root schedule I'm only
15:18
putting it in here because it was used
15:19
by one very high-profile paper in 2017
15:23
but it's like I've actually seen it used
15:24
compare
15:25
to be less compared to linear decay
15:27
schedules and the cosine learning rate
15:28
and the cosine decay schedules and I
15:31
think the the potential pitfall with
15:33
this inverse square root schedule is
15:35
that the model actually spends very
15:37
little time at that initial high
15:39
learning rate
15:40
so with this inverse square root
15:41
schedule you can see that the the
15:43
learning rate very quickly drops off
15:45
from its high initial value and then
15:47
spends a lot of time at these lower
15:49
later values and if we compare that with
15:51
the linear or the cosine schedule then
15:54
we see that in contrast with these with
15:56
these other schedules that are a bit
15:57
more popular models tend to spend more
15:59
time at those initial higher learning
16:01
rates and then I think with all this
16:05
talk about different learning rate
16:06
schedules I think I need to point out
16:08
another very probably the most common
16:11
real learning rate schedule is just the
16:13
constant schedule and this is actually
16:15
and this one surprisingly actually works
16:17
quite well for a lot of problems so here
16:20
you know we simply set some initial
16:22
learning rate and then keep that that
16:23
same learning rate through the entire
16:25
course of training and this is actually
16:27
what I recommend people do in practice
16:29
until they have some reason to do
16:31
otherwise where I see people mess up a
16:33
lot of times when they're starting new
16:34
deep learning projects is to fiddle use
16:37
this to fiddle with learning rate
16:39
schedules too early in the process and
16:41
typically fiddle changing learning rate
16:43
schedules should be something that you
16:44
do rather far along into the process of
16:47
developing your model and getting it to
16:48
work and you can usually get things to
16:51
work reasonably well just using a
16:53
constant learning rate schedule and then
16:55
the difference between performance with
16:56
a constant schedule versus performance
16:58
with one of these other more complicated
16:59
schedules is usually not the difference
17:02
between your model working and not
17:03
working usually moving from constant to
17:06
some more complicated schedule well
17:07
maybe make things work a couple percent
17:09
better so it's important if you're
17:11
really trying to push for the state of
17:12
the art on some problem but if your goal
17:14
is just to get something to work as
17:16
quickly as possible with this little
17:17
mess as possible then I think constant
17:19
learning rates are actually a pretty
17:22
good choice although I should also point
17:24
out that there is a bit of complication
17:27
between learning rates and the optimizer
17:29
that you choose so when using stochastic
17:32
gradient descent with momentum then I
17:35
think using some kind of learning rate
17:37
decay schedule is fairly important
17:39
but if we're using one of these more
17:40
complicated optimizers like rmsprop or
17:43
Adam then you can go farther using only
17:46
a constant learning rate so that's kind
17:48
of my caveat is that especially if
17:50
you're using something like Adam then
17:52
you can you you can actually get pretty
17:53
far using just a constant learning rate
17:56
so any questions about these learning
17:59
rate schedules before we move on to some
18:01
other topic yeah yeah so the question is
18:04
sometimes you'll train for a long time
18:05
loss will be going down you'll be very
18:07
happy because you think you trained a
18:08
good model and then all of a sudden loss
18:10
will go up things will explode and
18:11
you'll become sad yeah I think it's
18:13
really hard to say anything general
18:15
about that case I think there's a lot of
18:16
reasons why something like that can go
18:18
can go wrong
18:19
one case where I've run into a similar
18:21
problem is if you forget the torch got
18:24
zero grad that I warned you guys
18:25
multiple times about a couple lectures
18:27
ago
18:28
maybe that was not applicable to your
18:30
case but then if you're accidentally
18:32
accumulating gradients over many
18:33
iterations then things will tend to work
18:35
for a while but then after some point
18:37
then gradients will explode and things
18:39
will be things will go wrong I think
18:41
things can also blow up after depending
18:44
on the problem type that you're working
18:45
on you might also see bad training
18:46
dynamics so for something like different
18:50
types of generative models or especially
18:52
different types of reinforcement
18:53
learning problems then you'll often see
18:55
very troubling behavior with these
18:57
learning curves sometimes although if
18:59
we're kind of a standard well-behaved
19:01
classification problem if there's the
19:03
loss is kind of blowing up after a long
19:05
periods of training usually that
19:07
indicate that makes me suspect some kind
19:09
of a bug either bad hyper parameters or
19:11
some bug in the way data is being loaded
19:13
or maybe you're training on some kind of
19:15
corrupt data actually that I've seen
19:17
that as a problem sometimes where maybe
19:19
one sample in your training set is
19:20
actually corrupted one common cake one
19:23
common failure mode is like you know
19:24
Flickr actually takes users on can
19:27
upload images to Flickr or different
19:29
photo sharing sites and then later
19:30
decide to remove those images and then
19:33
many data sets are constructed not by
19:35
distributing actual JPEG files by
19:37
distributing links to Flickr images so
19:39
then if you go and naively try to
19:40
download all the links in the data set
19:42
you'll choose sometimes try to download
19:43
some images that have been removed by
19:45
users and then those will end up with
19:47
some kind of default corrupted JPEG file
19:49
and then during the training process
19:51
maybe it's an
19:52
Zero's with some non-trivial label and
19:54
then you'll explode when you try to
19:56
train on we train with a mini batch that
19:57
includes one of these corrupted image
19:59
files so sometimes sometimes data
20:01
corruption in your training set can
20:03
cause things to explode all of a sudden
20:05
but I think there's really no general
20:07
answer you need to dig in more to the
20:09
specifics of your problem ok but then
20:14
another strategy oh yeah yeah yeah so
20:16
the question is about adaptive learning
20:18
rates so that would be something like a
20:20
degrade or rmsprop or add-on are
20:22
examples of adaptive adaptive learning
20:24
rate mechanisms and they're I think
20:26
things can still blow up things can
20:28
still go wrong sometimes they tend to
20:30
make things a bit more robust but they
20:31
definitely still solve all your
20:32
optimization problems the question is
20:34
like oh maybe I want to write some
20:36
heuristic that will look at the lost
20:37
curve and then determine for itself when
20:39
the learning height will drop I've seen
20:41
people do this but I recommend against
20:43
it because I think you can get yourself
20:45
into trouble too easily doing that I
20:48
think it's very easy to try to code up
20:50
some clever solution that will try to
20:52
smartly choose when to drop the learning
20:53
rate but there's a lot of corner cases
20:55
that are difficult to account for for
20:58
example if you look at these curves
20:59
they're actually very noisy so here I'm
21:02
actually each actually I'm plotting up a
21:04
scatter plot with each dot being the
21:06
loss at a particular iteration but it's
21:08
so noisy that to get any signal you need
21:10
to take some kind of a moving average
21:11
over the the training iterations so then
21:14
they just end up being a lot of now sort
21:16
of meta hyper parameters that go into
21:17
these heuristics about when to decay the
21:19
learning rates and I think that you're
21:21
just again you're just setting yourself
21:23
up for trouble there and you're better
21:24
off just looking at the lost curves and
21:26
trying to make some expert determination
21:27
there yeah yes that's correct
21:31
so the dark is actually a bunch of
21:34
circles that represent the loss of each
21:35
iteration but they're extremely noisy so
21:38
these circles end up actually having a
21:40
few a pretty huge variance between
21:42
iterations on the individual training
21:43
losses so whenever I plot these things I
21:46
always like to plot both those losses at
21:48
every iteration to get some general
21:49
sense of the variance but then I also
21:52
like to plot a moving average so this is
21:54
usually a moving average of a window
21:56
size of like a hundred or a thousand
21:57
iterations that tells the moving average
21:59
of the losses over some some window and
22:01
that gives you a sense of both the
22:03
overall variance of the training
22:04
as well as the the longer-term trends so
22:07
I think that's very useful a very useful
22:09
thing when plotting losses to help you
22:11
debug and and absolutely point out that
22:16
this is kind of a very fairly
22:17
characteristic image that you get when
22:18
trading with a cosine schedule that it
22:20
has a very funny shape and if you're
22:21
used to seeing plots like this with the
22:23
stuff decay then the first time you
22:24
train with a cosine schedule you get
22:25
very surprised so this is something I've
22:27
started to do recently and and it's
22:29
always concerning to me when I see these
22:30
very weird-looking plots but I think
22:34
another thing that you should always be
22:36
doing that really helps you to choose
22:38
how long you should train is this notion
22:40
of early stopping and I think this is a
22:43
good mechanism to also go back to this
22:44
exploding process during training so
22:46
here the idea is that whenever you train
22:48
neural networks you want to look at two
22:51
to the really three curves one is the
22:54
training loss as a function of iteration
22:56
which is here on the left and if things
22:58
are healthy you should see this kind of
22:59
decaying exponentially in some way
23:01
but what you should also always be
23:03
looking at is the training accuracy of
23:06
your network maybe that you check every
23:07
every epoch or so as well as in the
23:10
accuracy both on the training set as
23:12
well as the validation set and the
23:14
looking at these curves can give you
23:16
some other sense of the health of your
23:17
network throughout the training process
23:19
so then what you'll typically do is you
23:22
you want to stop training you want to
23:25
pick the net the check point the check
23:27
point of the model during training where
23:28
you had the highest validation accuracy
23:30
so what you'll typically do is you'll
23:33
typically set some number of max
23:34
iterations or met max epochs that you're
23:36
going to train for and then just let
23:38
that thing train for that batch number
23:40
epochs but every epoch or every five
23:43
epochs or ten epochs you should always
23:44
check the valid the the training and
23:46
validation set accuracies and then save
23:49
the model parameters at that points to
23:50
disk and then after the model finishes
23:53
training then you can plot these curves
23:54
and then select the check point der it
23:57
just select the point in time at which
23:59
your model performed the best on the
24:01
validation set and then that's the check
24:03
point they should actually use when
24:04
about when running the model in practice
24:06
so if you do a process like this then if
24:08
the model happened to blow up late in
24:10
the training process then it's maybe not
24:12
such a big deal you can just look at
24:13
this curve and then pull one of the
24:15
model
24:15
check points from the point in training
24:17
before the model blew up so this is a
24:19
really useful skill a really useful
24:21
heuristic on how to train your networks
24:23
and how to select which check what to
24:25
use at the end of the day so this is
24:28
something I really encourage people to
24:29
use pretty much all the time whenever
24:30
you're training people networks so then
24:34
that kind of leads into a bit of a
24:36
larger discussion of how are we supposed
24:38
to go about choosing hyper parameters
24:41
for our neural networks well in kind of
24:44
1:1 sit one thing that you'll come and
24:47
we commonly see people do is this notion
24:49
of a grid search so here what we're
24:52
going to do is select some set of them
24:54
we're gonna select some set of hyper
24:56
parameters that we care to tune and then
24:58
for each of those hyper parameters we'll
25:00
select some set of values that we want
25:02
to evaluate for that hyper parameter and
25:05
for and often many of these hyper
25:07
parameters you should be searching in
25:09
kind of a log linear space rather than a
25:11
linear space so then for example we
25:13
might want to evaluate here for examples
25:16
of the learning rate for different
25:17
learning rates that are kind of spaced
25:20
out log in a log in a log minier way and
25:23
also test out for different values of
25:26
regularization strengths that again our
25:28
space sort of log linearly and then get
25:31
given four values of the weight decay
25:32
and four values of the learning rate
25:34
then that gives rise to 16 combinations
25:36
and if you have enough GPUs just try
25:38
them all and see which one works best
25:40
and that actually is a fairly reasonable
25:42
strategy that people sometimes do in
25:44
practice but the problem is that this
25:47
strategy requires a number of GPUs which
25:49
is exponential in the number of
25:51
hyperparameters that you want to tune so
25:53
this very quickly gets very infeasible
25:55
very quickly so another strategy that
25:59
sometimes people employ instead is
26:01
random search rather than grid search
26:03
and here again we're going to select the
26:06
procedure is much the same we're going
26:07
to select some set of hyper primers that
26:09
we want to tune and now rather than
26:11
selecting values that we want to try for
26:13
each of those hyper parameters instead
26:15
we're going to select some ranges of
26:17
those hyper parameters along which we
26:18
want to search and now we're going to
26:20
develop during each time we train our
26:22
model we're going to select a random
26:24
value for each of those type of
26:25
parameters that fall within that range
26:27
and again for
26:29
like a learning rate and a weight decay
26:30
you'll often want to search in a log
26:32
linear space whereas for other types of
26:34
hyper parameters like maybe with the
26:36
network or model size or dropout
26:38
probability sometimes you'll see a
26:40
linear rather than log rather than log
26:42
linear spacing and it kind of depends on
26:45
what the hyper parameter is as to
26:46
whether it should be linear or log
26:47
linear but now the idea is that with
26:50
this random search idea we set these
26:52
ranges for different hyper parameters
26:54
and then during each training run we
26:55
draw a random value for our high
26:57
parameter and then just let it go and
26:59
then however many of trials of your
27:02
network you can afford to train you
27:03
train that many and then whatever
27:05
happens to work best at the end that
27:07
gives you some that that's the high
27:08
parameters that you use and there's been
27:11
some maybe if you think about grid
27:13
search versus random search you know
27:15
they're 4-minute very similarly on the
27:17
slide so you should think that so maybe
27:18
you might think that they're very
27:19
similar in how they perform but there's
27:21
actually a fairly strong argument for
27:23
using random search instead of grid
27:25
search that comes from this 2010 paper
27:28
and the idea here is that if you have a
27:31
lot of hyper parameters and you don't
27:33
really know which hyper parameters are
27:35
in prop if you have a lot of high
27:36
parameters you need to tune probably
27:38
some of those hyper parameters are going
27:39
to be very important for model
27:41
performance whereas other of those hyper
27:43
parameters maybe it didn't really matter
27:45
what value you were gonna set they were
27:47
all anything in that range would have
27:48
been fine but ahead of time before you
27:51
train models you might not know which
27:53
hyper parameter is in which category but
27:56
usually it's the case that some hyper
27:57
parameters matter and some humber
27:59
parameters don't matter but now the idea
28:01
is that if you are using a grid search
28:04
then we were always going to evaluate
28:07
exactly the same grid of parameters on
28:09
the left so what this means is that in
28:12
this sort of cartoon picture the the
28:15
parameter on the horizontal axis is ends
28:19
up being very important for this
28:20
optimizing for getting good performance
28:22
because you can see that this sort of
28:24
distribution that we draw on the top of
28:26
the grid is sort of the the marginal
28:28
distribution of model performance as a
28:30
function of that hyper parameter value
28:31
so you can see that in this toy example
28:33
this horizontal hyper parameter is very
28:35
important for getting very good
28:37
performance because if we go far to the
28:39
left there's low performance and there's
28:40
kind of a sweet a small sweet spot
28:42
the middle that gives us very high model
28:44
performance in contrast in this sort of
28:46
cartoon picture the vertical hyper
28:48
parameter is maybe not so important for
28:50
model performance and you can you can
28:52
see we've also drawn this sort of orange
28:54
marginal distribution on the left-hand
28:55
side of the plot that shows that no
28:58
matter which value of this vertical
28:59
hyper parameter we choose things are
29:02
going to perform about the same and now
29:04
the problem is that if we do a grid
29:05
search we are not being very we're not
29:07
gaining as much information as we could
29:09
from each trial of the model that we
29:11
train because what we're going to try
29:14
the same values of the important type of
29:16
parameter many many times repeated for
29:19
each about for each value of the
29:21
unimportant parameter so what that means
29:23
is that for this important this
29:25
distribution of the important parameter
29:27
we're going only going to get three
29:29
samples in this cartoon example so that
29:32
means that maybe we don't have enough
29:33
information to properly tune the right
29:35
value of the right setting of that
29:36
important parameter now in contrast if
29:39
you're going to use random search on the
29:41
right than every but every trial that we
29:43
run is going to have random hyper
29:45
parameters for both the vertical and the
29:46
horizontal hyperparameters and what this
29:48
means is that when we plot these
29:50
marginal distributions of model
29:51
performance as a function of each hyper
29:53
parameter then we end up getting more
29:55
samples for each hyper parameter
29:57
individually because the points on the
29:59
grid don't align perfectly vertically or
30:01
horizontally so what that means is that
30:03
in a situation like this where one hyper
30:05
parameter is important and the other
30:07
peiper parameter is unimportant
30:08
then we end up getting using the
30:11
multiple sample the multiple repeated
30:12
samples of the unimportant hyper
30:14
parameter in order to give us more
30:16
effective samples of the important type
30:18
of parameter so then if you look at
30:19
these four though for the right-hand
30:21
plot giving this random grid search we
30:23
see we end up with many many samples of
30:25
this important type of parameter that
30:27
allows us to sample more points on this
30:29
curve and hopefully find a better value
30:31
overall so if you're in a setting where
30:34
you need to sort of search randomly or
30:36
private over hyper parameters then
30:38
usually using some kind of a random
30:39
search is much more important than using
30:41
some kind of a grid search so here these
30:45
couple these this slide is showing you
30:47
kind of a cartoon picture of some kind
30:49
of idealized situation of what might
30:51
happen with a random search but here's
30:53
an example of an ax
30:55
we'll random search that I did at a
30:56
project at Facebook so we could use a
30:58
lot of GPUs so here so here these plots
31:02
we were evaluating the the learning rate
31:04
and the regularization strength for
31:07
three different categories of models
31:09
that would be a feed-forward model a
31:10
residual model and a different sort of
31:13
model architecture called darts that
31:14
details of what that is is not important
31:16
for this purpose but what you can see is
31:18
that each point on these plots is a
31:20
different model that we trained and the
31:23
plot is quite dense as we trained a lot
31:24
of models and then the the color of the
31:27
point gives you the the overall
31:30
performance of the model at the end of
31:31
training and by looking at plots like
31:34
this you can get some sense of the
31:35
interactions between different learning
31:37
rates that you might come across so what
31:40
you can see here is that here the the
31:42
x-axis is a learning rate and the y-axis
31:44
is the regularization strength along a
31:47
log scale and what we can kind of see is
31:49
that there is some kind of non-trivial
31:51
interaction between these two parameters
31:53
but there's this kind of sweet spot or
31:55
sweet River in the middle of good
31:57
learning rates for each regularization
31:58
strength and vice versa
32:00
yeah was there a question yeah the
32:01
question is should can you use gradient
32:04
descent to learn the hyper parameters
32:06
yes you can and I think that's that's a
32:09
really cool area of research that I
32:11
really enjoy and I think it's really
32:13
creative and really interesting so
32:15
there's many different approaches to
32:16
that that I think are slightly beyond
32:17
the scope of this lecture but to give
32:20
you a flavor of what that looks like I
32:21
think that's actually a beautiful
32:23
situation where the software systems
32:25
that we end up building to help us solve
32:27
our problems end up giving rise to new
32:30
mathematical solutions as well and what
32:32
I mean by that is when you have
32:33
something like PI torch it's very easy
32:35
to back propagate through arbitrary
32:37
Python code and you know your
32:39
optimization is your optimization loop
32:41
is again yet again just another bit of
32:43
Python code so in principle it's very
32:46
easy in pi torch to write code which
32:49
will allow you to back propagate to sort
32:51
of have to an inner loop and an outer
32:52
loop in the inner loop you're going to
32:54
run optimization over your model
32:55
parameters but you'll actually back back
32:57
propagate through that entire inner loop
32:59
in order to compute gradients of the
33:01
final model performance on the initial
33:03
values of the hyper parameters and
33:05
that'll and then in this outer loop then
33:07
you'll use gradient
33:08
to learn the hyper parameters and
33:10
there's a bunch of really really cool
33:11
papers along this direction that I would
33:13
love to find some way to sneak into one
33:15
of these lectures but then there's one
33:18
paper I love there where they learn not
33:19
only the learning rates but they also
33:21
use similar idea to learn the training
33:23
data because now if you can back
33:25
propagate through the learning process
33:27
we can actually learn the optimal
33:29
training set that will cause the trained
33:31
model to work well on the validation set
33:32
oh so this is like very crazy and very
33:35
very very very fun to read papers in
33:38
this direction so yes you can but it's
33:41
actually not commonly used in practice
33:43
these things are super computationally
33:44
expensive and people only sort of at
33:46
this point in time employ them for
33:48
relatively small toy problems to show
33:50
off that they can but for very large
33:52
scale problems of these kind of
33:54
automatic methods of learning hyper
33:55
parameters be a gradient descent are
33:57
really not very practical to scale up to
33:59
very large scale problems yeah yeah the
34:02
color scale is error rate but it looks
34:04
like this the color bar on the right
34:06
doesn't quite match up to actual colors
34:07
in the plot so I apologize for that but
34:10
clearly the point where we put the red
34:12
dot or the values we ended up choosing
34:13
for the paper so that got to be the
34:15
highest one so you can see this dark
34:17
purple means it's working well and then
34:19
moving towards yellow is things not
34:20
working well so I think there's probably
34:22
some transparency issue between the
34:24
actual color bar in the plot maybe I
34:26
need to talk to my co-author about that
34:29
so but so this is this that's kind of a
34:32
good strategy to choose hyper parameters
34:34
if you happen to be working at Facebook
34:36
or Google or another tech company I have
34:37
access to a lot of GPU resources but if
34:40
that's not the case then you need to be
34:42
a little bit smarter in the way that you
34:43
choose hyper parameters so this is but I
34:47
think you should not despair it's
34:49
usually possible in my experience to
34:51
choose pretty good hyper parameters for
34:52
your problem
34:53
without this massive massive hyper
34:55
parameter search so this is the
34:57
procedure that I usually go about
34:59
choosing hyper parameters when I don't
35:01
have access to a very large GPU cluster
35:03
so the step one is that you ahve you
35:06
implement your model actually you need
35:07
to like write some code first and once
35:09
you're done with that then you should
35:10
always be checking your initial loss and
35:12
as we talked about multiple times so far
35:14
usually by the structure of the loss
35:16
function that you're using you can kind
35:17
of compute analytically what what sort
35:19
of initial lost you expect
35:21
at random initialization so for
35:23
something like this cross entropy loss
35:24
you know it should be like minus log of
35:26
the number of classes so then your first
35:28
step after implementing your model is
35:30
you turn off weight decay and just check
35:32
this loss at initialization that only
35:34
takes one iteration it's very cheap to
35:36
do very fast and if that loss is wrong
35:38
you know you have a bug and you should
35:39
go back and fix the bug then the next
35:42
step is to try to overfit a very small
35:45
sample of your training data so here the
35:47
idea is that you want to take something
35:49
like between one and maybe five to ten
35:51
mini batches of data like a very very
35:54
tiny sample of your training set and now
35:56
try to overfit this to a hundred percent
35:57
and because and when you're doing this
36:00
you always want to turn off the
36:02
regularization and your goal is just to
36:04
over fit the training data and when
36:06
you're doing that then what you need to
36:08
do is fiddle with the the precise
36:09
architecture of your model maybe the
36:11
number of layers and the size of each
36:12
layer play with the learning weight will
36:14
play with the method of weight
36:16
initialization and now when you're when
36:18
you when you play around with these
36:19
different some of these different hyper
36:21
parameters then you should be able to
36:22
usually get the get whatever model
36:25
you're working on should be able to get
36:27
to like 100 percent accuracy on this
36:29
very very tiny sample of the training
36:31
set within a very small amount of time
36:33
your goal here is that you should be
36:35
able to over fit in something like five
36:36
minutes of training and because you're
36:39
this sample training stuff you're
36:41
working on is very very small and now
36:42
because the training times are very very
36:44
short it allows you to kind of
36:45
interactively play around with different
36:47
values of these settings in order to
36:49
find settings that cause you to overfit
36:50
very quickly and the point of this stuff
36:52
is to just make sure that you don't have
36:54
any bugs in your optimization loop
36:56
because if you can't over fit 10
36:59
training examples or 10 batches of data
37:00
then you have no hope in actually
37:02
fitting the training set for real and
37:05
it's surprising how often you can catch
37:06
bugs in your optimization setup or in
37:08
your model architecture choices just in
37:11
this stage and again this these training
37:13
loops run very very fast so you can do
37:15
this interactively on a single GPU in
37:17
most cases and and and there in step two
37:21
we don't care about regular we don't
37:22
care about generalization to the
37:23
validation set at all we're just trying
37:25
to debug the over the optimization
37:26
process on a small training set but once
37:29
we've done that then once we have to
37:31
succeed it at Step two and be able to
37:33
overfit a very small amount of data
37:35
then we want to do is take the
37:37
architecture from the previous step and
37:39
now use all of the training data and now
37:42
your goal is to find a learning rate
37:44
that will cause the loss to start to go
37:46
down quickly on your whole training set
37:49
now hopefully from step two you found it
37:51
you you know that your code is correct
37:53
you know that your optimization loop is
37:54
correct and you found a model
37:56
architecture that you believe is
37:57
sufficient for modeling your data and
37:59
now in step three you're going to take
38:01
all of those architectural parameters
38:04
and copy them over and you're just going
38:05
to fiddle with the learning rate on the
38:07
entire training set and the learning
38:09
rate is the only parameter you'll the
38:10
alerting rates the only parameter you'll
38:11
change and in changing the learning rate
38:14
your goal is to make the loss drop
38:15
significantly within the first say
38:17
hundred iterations of training or so
38:19
because usually for most problems that
38:22
are set up properly usually you'll see
38:24
some very high initial loss at the
38:26
beginning and you'll tend to see some
38:27
kind of exponential decrease in loss
38:29
within the first hundred to thousand
38:31
iterations of training and that's sort
38:33
of empirically true across a wide
38:35
variety of problems and neural network
38:36
architectures so at this K at this stage
38:39
again because you're only caring to
38:40
train for something like a hundred or a
38:42
thousand iterations and again you can
38:45
typically do this interactively and just
38:47
choose learning rates look at the
38:48
learning curve and then based on what
38:50
the plots look like then go back and
38:51
choose new learning rates and work
38:53
interactively until you can find a
38:54
setting of the learning rate that causes
38:56
things to actually converge within the
38:58
first start to converge within the first
39:00
100 or so iterations so now at this
39:03
point we're in relatively good shape
39:04
we've got a architecture that we know
39:07
has the potential to model our data
39:08
because it can over fit a couple
39:09
training samples and we know our
39:11
optimization is in a pretty good state
39:13
because we know it loss is starting to
39:15
go down at the beginning of training so
39:17
now step four is to set up a very coarse
39:20
hyper parameter grid maybe like a very
39:23
very small number of models maybe you
39:25
choose two different values of learning
39:27
rates and two different values of
39:28
regularization strength or just choose a
39:30
very very very tiny hyper parameter grid
39:33
to evaluate that is somewhere in the
39:35
neighborhood of all the choices that you
39:36
have made up to this point and now
39:38
hopefully because of the by following
39:40
these previous steps hopefully all of
39:42
these hyper parameter choices within
39:44
this very small grid will all end up
39:46
being somewhat reasonable
39:48
you will not have any catastrophic ly
39:49
bad models within this very type of tiny
39:51
hyper parameter grid so then after you
39:53
have this course grid then you train on
39:55
the full training set for something like
39:57
one to five or one to ten he pops and
40:00
that should and that should be enough to
40:01
give you some sense of the
40:02
generalization performance of your model
40:04
beyond the training set and actually see
40:05
it start to look at how it performs on
40:07
the validation set and then at that bend
40:10
and again this is something that you
40:11
probably cannot do interactively but at
40:14
this point you've got enough familiarity
40:15
with your model but you know that all of
40:17
these choices should hopefully work so
40:19
you set up this tiny hyper parameter
40:20
grid depending on how many models you
40:22
can afford to train in parallel and then
40:23
step back and then come back and I'll
40:25
come back after a coffee break or a
40:26
night of sleep or a weeks vacation as
40:28
the case may be depending on how long
40:30
your model takes to Train and then come
40:32
back and see how well these things did
40:35
and then after you go to step four
40:38
then you enter this iterative loop of
40:40
looking at the results of your previous
40:42
tiny hyper parameter grid and then
40:44
adjust your hyper parameter grid and
40:46
then go back and train for longer and
40:48
then see and then at this point you're
40:50
in this sort of interactive process but
40:52
each iteration of this of this of this
40:54
procedure might take hours two days
40:56
depending on exactly how long your model
40:58
takes to Train so always along the way
41:01
your you're looking at the learning
41:02
curves and then use it and using that to
41:04
make determinations about how you should
41:06
change your what time the changes you
41:09
should make and your hyper parameter
41:11
grid going forward so when I say look at
41:14
learning curves I mean that we need to
41:16
look at these plots that I mentioned
41:18
earlier on that whenever you're looking
41:20
at these things you should always be
41:21
plotting this training loss on the left
41:23
and again as I said I like to plot both
41:26
the raw accuracies as a scatter plot as
41:28
well as a line plot of these moving
41:30
average of losses and then on the right
41:32
I always like to plot the training and
41:34
validation accuracies and check every
41:37
Deepak and then by looking at these
41:39
learning curves you you can get usually
41:42
gained some sense about what could be
41:43
going right or wrong with your model so
41:46
here so here I'll give you a couple
41:48
cartoon pictures of different sorts of
41:50
shapes of learning curves that you
41:52
should become familiar with so one
41:54
situation is when your learning curve
41:56
looks something like this where it's
41:58
sort of very flat at the beginning and
41:59
then makes a sharp initial drop
42:01
well if you see a shape like this that
42:03
means that probably your initialization
42:04
was bad because the the law you were not
42:07
making progress at the beginning of
42:09
training so you should often adjust your
42:11
initialization and go back and try again
42:13
now another problem is when we see a
42:15
loss like this where it's sort of making
42:17
good progress and then it plateaus after
42:19
a while well and when you see a loss of
42:22
the lost curve like this that you should
42:24
consider some kind of learning rate
42:25
decay because it's possible you're
42:27
learning right was too high and maybe
42:29
around the around the time point where
42:31
it tends to plateau is maybe the point
42:33
where you should introduce some kind of
42:34
learning rate decay and lower the
42:36
learning rate now conversely if you have
42:39
not followed my advice and have
42:41
introduced learning rate decay too early
42:43
in the model development process then
42:45
you might see a learning curve that
42:46
looks something like this so here the
42:48
model was making good progress and then
42:50
at some point we hit our step decay or
42:53
our point at which we were applying a
42:55
step decay on the learning rate and then
42:57
the loss made a small drop at the point
42:59
where we stepped and then after that it
43:01
was completely plateaued now usually
43:03
this means that you decayed too early
43:05
that if you look at the shape of the
43:07
learning curve leading up to the point
43:08
in time where we apply the decay then
43:10
you think then this looks like the loss
43:13
would have continued going down at the
43:15
initial learning rate had we not applied
43:17
learning rate decay so this is a shape
43:20
that you should watch out for and
43:21
usually means that you applied learning
43:23
rate decay too early now these were all
43:26
shapes of the this moving average of
43:28
training losses but you can also gain
43:30
some intuition by looking at these plots
43:32
of training and validation accuracy over
43:35
time so one character one characteristic
43:39
shape of these curves that you might see
43:41
here is often that they'll make some
43:43
exponential increase at the beginning
43:45
and then they'll sort of slowly increase
43:47
linearly over time now if you see a
43:49
shape like this there's some kind of
43:52
non-trivial but maybe healthy gap
43:54
between train and bowel and they're both
43:57
continuing to go up then when you see a
43:59
curve like this it means things are
44:01
going well and you just need to train
44:02
for longer because it looks like these
44:04
curves are still going up and hopefully
44:06
just do whatever you're doing keep doing
44:07
it for longer and your models will
44:08
continue getting better so this is a
44:10
curve that you like to see in these
44:12
training Valpo
44:13
now a plot like this means something
44:16
very bad is going on so here this is a
44:18
characteristic plot of overfitting so
44:20
here you can see that performance on the
44:22
training set has continued to increase
44:24
over time but performance on the
44:26
validation set has either plateaued or
44:29
even decreased over time now this
44:31
usually is some no it's it's very common
44:34
to have some kind of a gap between train
44:36
and Val that's normal and healthy but
44:38
when you see this very large and
44:39
increasing gap between train and Val
44:41
that is a sign of overfitting and when
44:44
you see a learning curve like this that
44:47
means that you need to either increase
44:49
your regularization strength collect
44:51
more training data or maybe in some rare
44:55
cases decrease the size or capacity of
44:57
your model but this is this is a
44:59
characteristic shape of overfitting in
45:03
in contrast if you see a plot like this
45:05
where the training performance and the
45:06
validation performance is almost exactly
45:08
the same you might think this is a good
45:11
thing because there's no overfitting but
45:13
usually this is a bad sign usually if
45:15
you see exactly the same performance on
45:17
the training set and the validation set
45:19
usually that means you are actually
45:21
under fitting your data and in fact you
45:23
would have been better off to increase
45:25
the capacity of your model or decrease
45:27
the regularization and that will tends
45:29
to give you better overall performance
45:31
on the validation set even if young even
45:34
if that results in a larger gap so this
45:36
one is slightly kind of counterintuitive
45:37
that this is actually an unhealthy
45:40
learning curve and usually means that
45:41
you are under fitting the training data
45:43
any questions about these learning
45:45
curves yeah yeah the question is um can
45:48
you also tell by looking at the absolute
45:49
accuracies and if they're particularly
45:51
low and yeah that's also definitely a
45:53
good sign to know whether you're not
45:55
you're under fitting the data but there
45:57
it requires you to have some prior
45:58
knowledge about what is a reasonable
46:00
accuracy on this data set which you
46:01
might not coming into a new problem well
46:04
so usually I mean this is kind of
46:05
empirical so usually it turns out that
46:08
you know what we want is a model that
46:10
achieves the best on unseen data and it
46:12
happens I mean I think there's not a
46:14
strong theoretical reason for this but
46:16
the empirical fact is that usually when
46:18
you find a model that achieves the best
46:20
accuracy on the unseen data then there
46:22
typically is some kind of a non-trivial
46:24
gap between performance on the training
46:26
and performance on the validation set
46:28
and that's kind of an empirical fact I
46:30
think there's not really great theory
46:31
that I can point to you for that problem
46:32
for that uh for that observation so then
46:37
this brings us to our final close our
46:40
final step in this type of parameter
46:42
training policy you look at these lost
46:44
curves and then based on your intuitions
46:46
about how things are going
46:47
that should gives you some sense about
46:48
how to adjust your grids based on
46:50
looking at these lost pots that we just
46:51
looked at then you go to step 5 and loop
46:54
until you run into your paper submission
46:55
deadline and you have no more time to
46:57
train models so basically what I like to
47:02
think about when you're tuning these
47:04
things is that you're some kind of a DJ
47:05
tuning all these little knobs about your
47:07
learning rate strength and your hyper
47:09
parameters strength and your dropout and
47:11
your model architecture and then
47:13
hopefully if you tune all these knobs in
47:15
just the right way you'll end up making
47:16
beautiful music in the form of a model
47:18
that works really well and unseen data
47:20
and in order to do that it's often very
47:23
helpful to set up some kind of a cross
47:25
validation command center where you need
47:27
where you can look at very like you need
47:29
to train large numbers of models in
47:30
parallel and then look at these learning
47:32
curves in parallel and then use this as
47:35
a way to get some idea about what sets
47:37
of hyper parameters tend to be working
47:38
well or tend to not be working well now
47:41
back in the day before things like
47:42
tensorflow this was a pain in the butt
47:44
and you had we had to like write custom
47:46
web code in order to visualize these
47:48
learning curves and like learn
47:49
JavaScript plotting frameworks to plot
47:51
these things or set up your own custom
47:54
jupiter notebooks for plotting these
47:56
things and you could end up spending a
47:58
lot of time just on the infrastructure
48:00
of looking at the results of your
48:01
experiments but now with things like
48:04
tensor board you a lot of that work has
48:06
been done for you so it's usually a lot
48:09
more seamless nowadays to set up these
48:11
kind of cross validation command centers
48:13
as they as you will another there's
48:17
another cut there's some other
48:18
heuristics that you can sometimes look
48:19
at that can help you diagnose things
48:21
that are going wrong and training so for
48:24
example one thing you sometimes like to
48:25
do is look at the ratio between the
48:27
values of the weights of your network
48:29
and the and the magnitudes of the
48:31
updates that you're making onto those
48:33
same ways so that would be you know your
48:35
gradients times their learning rates
48:36
gives you the overall delta that you're
48:39
going to use
48:39
update each value of the way that each
48:41
iteration and generally speaking you
48:43
want to AP's the value between the
48:45
absolute value of the weight and the
48:47
absolute value of the update for each of
48:49
the scalar weights in your network to be
48:51
not too large typically if you're making
48:53
updates that are of the same order of
48:55
magnitude or larger in order in larger
48:57
in magnitude then the weight value
48:59
itself that's usually some kind of a
49:01
sign that something bad is happening so
49:03
looking at these ratios between weight
49:06
update magnitudes and weight magnitudes
49:07
is sometimes some heuristic that people
49:09
look at and practice or maybe looking at
49:12
other kinds of statistics of the
49:14
gradient and magnitudes or magnitude is
49:15
something that can sometimes help you
49:17
debug problems that are going wrong
49:18
during training so that gives us to
49:22
looking at these training dynamics and
49:24
hopefully if you follow this simple step
49:25
a seven step procedure that I've
49:27
outlined you'll be able to train really
49:29
good models even if you don't have
49:30
access to a giant GPU cluster but then
49:33
the question happens is that after your
49:35
after you've successfully trained some
49:37
models then what can you do after that
49:39
now well now things get interesting
49:41
so one thing that you often want to do
49:43
is to get a little bit better on your
49:46
train on your on your final test set and
49:48
it turns out there's a very hip simple
49:50
heuristic that apply is almost across
49:51
the board for getting a slightly better
49:53
performance on basically whatever
49:55
problem you're considering and that is
49:57
that you train something like n
49:58
independent models however many you can
50:00
afford to train and then rather than
50:02
using one of them instead you just use
50:04
all of them at test time so that means
50:06
for each sample in your test set you run
50:08
it through each of your trained models
50:10
to get the predictions from each of your
50:12
trained models and then you average the
50:14
predictions across all of your trained
50:16
models for something the exact mechanism
50:18
of averaging kind of depends on the
50:20
exact problem you're trying to solve but
50:22
for something like image classification
50:23
you could for example take an average of
50:25
the probability distributions that are
50:27
output from each of the models because
50:28
the average of probably distribution is
50:30
still a probability distribution and
50:32
typically when you take an ensemble of a
50:35
bunch of different models you end up
50:36
getting about one or two percent better
50:38
on your final test case on your final
50:40
test set so that is pretty standard no
50:44
matter what the model architecture is or
50:46
how many models you're honest not well
50:47
more is usually better but what tasks
50:49
you're working on what data set you're
50:51
working
50:51
what's your underlying CNN architecture
50:53
typically you get about one to two
50:55
percent better when you ensemble some
50:57
some set up models together so if you're
51:00
really trying to squeeze out that last
51:01
bit of juice then this is a very common
51:03
trick that you'll see people use one
51:06
kind of cute idea is that rather than
51:08
training multiple independent models
51:09
sometimes you can get away with saving
51:12
multiple check points of one model
51:14
during training and then actually
51:16
average the results of those different
51:18
check points during training and that
51:19
can also give you some some improved
51:22
performance and then one trick there is
51:24
actually to Train with a very very
51:25
bizarre learning rate schedule that is
51:27
actually periodic so this is like not
51:29
super mainstream but it's so crazy I
51:31
wanted to point it out the idea is that
51:33
your learning rate schedule is now
51:34
actually periodic but it starts high
51:36
goes low goes high again goes low again
51:38
goes high again goes low again and then
51:40
the the check points that you take
51:42
during training to form your kind of
51:43
ensemble are the values of the model
51:46
weights that were at the the very low
51:47
point of each of those points in the
51:49
learning rate schedule that's kind of a
51:51
cute idea that you might see people use
51:53
sometimes another idea is to keep a
51:56
running average of the model weights
51:58
that you see during training and this is
52:00
called polyak averaging and it's used
52:02
actually pretty commonly in some large
52:04
skill generative models so here remember
52:06
like in batch normalization we're always
52:09
keeping this running exponential average
52:11
of the the means and the variances of
52:13
our features and then during testing we
52:15
we use those that this exponentially
52:18
running mean of means and standard
52:20
deviations for batch normalization
52:21
during test time well it turns out you
52:24
can actually do the same thing with the
52:25
model weights so then rather than using
52:27
the model weights that result from any
52:29
one iteration of gradient descent
52:30
instead you can take an exponentially
52:32
running average of the model weights
52:34
that you see during training and then
52:37
actually use the this exponential
52:39
running average of the model weights at
52:41
test time instead and this can have the
52:43
effect of helping you to smooth out some
52:45
of this iteration to iteration variation
52:47
in the model that happens during
52:49
training right if you go back and look
52:50
at these lost plots remember there was a
52:52
lot of variation in the loss between
52:53
individual iterations of the model and
52:55
by applying some kind of sum this is
52:57
this kind of averaging to the model
52:58
weights themselves it can help to
53:00
average out some of that noise that
53:02
happens between individual iterations of
53:03
SGG
53:06
so those are ways that you can squeeze
53:08
out just a little bit of extra juice on
53:09
whatever is the original task you were
53:12
trying to solve was but there but
53:15
sometimes we actually want to use one
53:17
trained model to help us solve a totally
53:20
different task and that is extremely an
53:23
extremely powerful tool that has become
53:24
super mainstream in computer vision over
53:26
the past several years and that's
53:29
basically the problem of transfer
53:30
learning so here the idea is that so
53:34
here there's kind of a myth that goes
53:35
around when training CN NS the myth is
53:39
that you'll often see people to see
53:41
people say is that you need very very
53:43
large training sets if you want to
53:45
successfully use deep learning for your
53:47
problem but I think this is actually
53:49
false and I'd like to bust this myth so
53:52
the idea is that I think if you utilize
53:53
transfer learning you can actually get
53:55
away with using deep learning for a lot
53:57
of problems even in cases where you do
53:59
not have access to a very large training
54:01
set so for this reason transfer learning
54:03
has become a critical part of pretty
54:05
much all mainstream computer vision so
54:08
the basic idea is that we'll take step
54:10
one is train a convolutional neural
54:13
network model on imagenet or some other
54:15
a very large scale image classification
54:16
data set and then make that part work as
54:20
well as you possibly can using all the
54:21
tricks that we've outlined and then step
54:23
two is is to sis to realize that we
54:27
don't actually care about image net
54:28
classification performance instead we
54:30
might care about classification
54:31
performance on some other smaller data
54:33
set or some other task entirely well
54:36
here the idea is that we will take our
54:38
trained network from image data and then
54:40
remove the last fully connected layer
54:41
recall that for example the last fully
54:44
connected layer in something like an
54:45
Alex net takes us from these 4096
54:48
dimensional features into this a 1000
54:50
dimensional vector of class scores so in
54:53
fact this last layer and then in the
54:55
network ends up being tied to the
54:57
category identities of the categories on
54:59
which the model was trained but now what
55:02
we can do is simply throw away that last
55:04
layer and delete it from the network and
55:06
just use those 4096 dimensional vectors
55:09
at the second-to-last layer of the
55:11
network as some kind of general feature
55:13
representation of our images and then
55:16
you can just you run our like freeze
55:19
whole weights of the network and just
55:20
use those extracted features as or
55:22
represent eighth as a feature vector
55:24
that represents your images and what
55:27
people found out starting in about 2013
55:29
22 2014 was that this seemingly simple
55:33
idea allows you to get very good
55:35
performance on many many computer vision
55:37
problems so for example there was
55:40
another data set that was called Caltech
55:42
101 that was unsurprisingly 101 object
55:45
categories but overall a lot lot smaller
55:48
in size and image net and here what
55:51
we're showing is the red curve so here
55:53
the x axis shows the number of training
55:55
samples on Caltech 101 that we're using
55:57
per category and the y axis as though as
56:00
the classification performance on this
56:02
Caltech 101 dataset and now the red
56:04
curve was this a prior stated prior pre
56:07
deep learning method that was that was
56:10
the state of the art on Caltech 101 that
56:12
was very particularly design set a
56:14
feature extraction pipeline for this
56:16
particular data set and now the the blue
56:19
and the green curves show this very
56:21
simple procedure of taking an Alex nest
56:24
that was pre trained on image met and
56:26
then using the final the second to last
56:29
layer of alex net features as this
56:31
predefined feature vector and then
56:33
simply training a logistic regression
56:36
an SVM in this case so they trained
56:39
either a logistic regression or a
56:41
support vector machine which of these
56:42
simple linear models that work on top of
56:45
this fixed 4096 dimensional feature
56:46
vector that was extracted from our pre
56:48
trained model and what they found is
56:50
that using this very simple procedure of
56:52
training a linear model on top of these
56:54
pre extracted feature vectors they were
56:56
able to significantly outperform the
56:58
state of the art in this data set and in
57:00
particular they were able to get
57:01
non-trivial performance even using
57:03
something like five to ten samples a per
57:06
class on this new data set so this
57:09
actually is very common and if you are
57:12
able if you use an image net pre trained
57:15
model to extract features then you can
57:17
tend to get reasonably good performance
57:19
on many data stream tasks even when you
57:21
don't have a very large training set and
57:23
this is definitely not particular to
57:25
Caltech 101 we saw that this was also
57:28
similar on this other bird
57:30
classification data set at that time
57:32
so here DPD and Pio and POF poof were
57:37
existing methods that were very
57:39
particularly tuned for this task of
57:41
recognizing birds and what they found is
57:43
that by simply training a logistic
57:45
regression type of these pre extracted
57:47
features from an Alex that then they
57:49
were able to outperform these previous
57:51
methods and if they were and by
57:53
incorporating the Alex net features into
57:56
the previous method they were able to
57:57
get an even larger boost and this was
57:59
simply swapping out the Alex net
58:01
features for whatever else that previous
58:03
method was doing in their learning
58:05
process and this applies across not just
58:09
a image Caltech 101 and birds this apply
58:11
this this tends to apply across a very
58:13
large set of image classification
58:16
problems so there was another another
58:19
another paper benchmarked this image
58:21
this this idea of extracting features
58:24
and then training linear models on top
58:26
of them for a whole suite of different
58:28
image classification problems so here
58:30
you can see they got good performance on
58:32
objects scenes birds flowers human
58:35
attributes and object attributes and in
58:37
all cases they were able to outperform
58:38
the previous state of the art on those
58:40
data sets and what's astounding here is
58:43
that each of those blue bars which is
58:45
the previous state of the art on one of
58:47
those data sets was typically a
58:49
completely independent method that had
58:51
been tuned independently for that one
58:53
particular data set and and here they
58:56
were able to use this very very simple
58:58
procedure to you that utilizes fine
59:00
tuning and outperform all of these
59:02
different methods using one simple
59:04
procedure of extracting extracting
59:06
features from a pre trained model on
59:08
imagenet and then simply training linear
59:10
models on top of those for the
59:11
downstream tasks and this applies not
59:14
only to image classification but it
59:16
turns out that this idea of utilizing
59:17
features from a pre trained Network
59:19
applies to a wide variety of image
59:21
computer vision problems as well so that
59:24
same paper in it I guess beating one two
59:27
three four five six state-of-the-art
59:29
methods wasn't enough for them they also
59:30
benchmark a set of image retrieval tasks
59:33
so the details is not super important of
59:36
how these works but basically the setup
59:38
is that you for example get an image of
59:41
some building at Oxford and your task is
59:43
to based on the pics
59:44
of the one building retrieve other
59:46
images from a database that are other
59:48
images of the same building maybe from a
59:50
different viewpoint or different time of
59:51
day or something like that
59:53
so this is some kind of a like search by
59:55
image you know if you upload an image to
59:56
Google and then search for similar
59:57
images and again here by using these
60:00
here the idea is that we're going to
60:03
extract these features from our models
60:06
that are pre-trained on image dat and
60:08
then we will simply use near some kind
60:10
of simple nearest neighbor procedure to
60:12
perform this image retrieval tasks and
60:14
again by using simple nearest neighbors
60:16
on top of these pre-trained feature
60:18
vectors from image that they were again
60:20
able to outperform a large number of
60:22
previous methods on a large set of image
60:24
retrieval datasets so this this is
60:29
probably the simplest example of this
60:31
transfer learning tasks where we simply
60:33
extract feature vectors and then use
60:35
them out of box and for some either a
60:37
linear about that or a retrieval
60:38
baseline yeah question yeah so that yeah
60:41
thanks for pointing it out so for this
60:43
paper in particular they had another
60:45
trick in their bag which was actually
60:46
applying data augmentation to the raw
60:48
images before the extracted feature
60:50
vectors and they found that this was
60:52
actually pretty important for for
60:53
beating these pipe methods but again
60:56
this data augmentation is fairly simple
60:58
and very fairly straightforward
60:59
these are mostly these random scales and
61:01
flips and crops that we kind of talked
61:03
about in a previous lecture and then
61:05
they kind of take it ensemble over many
61:06
different test time taking ensemble over
61:08
many different random data augmentations
61:11
for the data point but again it's sort
61:13
of a fairly simple straightforward
61:14
procedure that is very the same across
61:16
all the data sense so that's kind of the
61:21
simplest procedure where we simply use
61:23
the pre trained Network and just use to
61:25
extract feature vectors out of the box
61:27
and plug those feature vectors into some
61:29
other algorithm but if your dataset is a
61:31
little bit larger you can often do
61:33
better than that using this procedure we
61:35
call fine tuning so here the idea is
61:37
that we will take our image our model
61:41
which has been pre trained on something
61:43
like image net and then maybe throw away
61:45
the last layer and reinitialize the last
61:47
layer to be maybe a new layer that
61:49
pertains to the classification
61:50
categories on our new data set and now
61:53
we will continue training the entire
61:55
model for our new
61:57
our new classification data set and in
61:59
this problem and then rather than just
62:01
using it as a fixed feature extractor
62:02
we'll actually back propagate into the
62:04
model and continue updating the weights
62:06
of the model to continue improving
62:08
performance on this downstream task and
62:11
here there's a couple tricks and tips
62:12
one is that sometimes you often need to
62:16
reduce the learning rates a lot when
62:17
you're doing this fine tuning and
62:18
another trick is that sometimes you
62:20
might want to first start from
62:22
extracting features and then getting a
62:24
linear model to converge on top of
62:26
features and then after you do that go
62:27
back and fine-tune the whole model so
62:29
those are maybe some tips and tricks are
62:31
on this fine tuning that you might do in
62:32
practice and it turns out that this
62:35
procedure of fine tuning can actually
62:37
give pretty substantial gains in
62:39
performance of a lot of tasks so here
62:41
for this task of object detection that
62:44
we'll talk about in more detail in a few
62:46
lectures so you don't need to know how
62:48
this works or what is what this number
62:50
on the vertical axis is just higher is
62:52
better at 100 is perfect so what this
62:55
means is that the blue bar is using some
62:57
kind of transfer learning for object
62:59
detection on two different data sets
63:00
this blue bar was using fixed feature
63:03
vectors where we just freeze the entire
63:05
network and use it as a fixed feature
63:07
extractor and now the orange bars are
63:09
where we actually continue training the
63:11
entire neural network model on the new
63:13
on the new data set and what we can see
63:15
is that by by a fine tuning actually
63:19
things are working a lot better than
63:20
this this gives us a huge boost over
63:22
just using the network as a feature
63:23
extractor another point here is that the
63:27
architecture that you use matters so I
63:30
told you that step one was to train a
63:31
model on imagenet well it turns out that
63:34
the model that you train matters a lot
63:35
and in general models that work better
63:39
on imagenet tend to also work better for
63:41
many many other computer vision problems
63:43
so this is why this is I think the
63:46
reason why many people in computer
63:48
vision and basically everyone in
63:50
computer vision knows the exact relative
63:52
order of all the models on image net and
63:53
the reason for that is not because
63:55
they're obsessed with the image next
63:56
challenge it's instead because models
63:59
that work better on image net for many
64:00
years tended to also work better on
64:02
basically every other problem that you
64:04
tried so there was a period in time when
64:07
basically he would just take the best
64:10
the
64:10
latest and greatest model that worked
64:11
the best on imagenet and just apply it
64:13
to whatever your problem at hand was and
64:15
things would would get better an example
64:19
from that comes from again this object
64:20
detection problem so it's it's very
64:23
difficult to find papers that make
64:25
controlled comparisons between different
64:26
imagenet models so this object detection
64:29
comparison is not perfect but it's the
64:31
closest I could find so here again the
64:34
y-axis is the performance on this object
64:36
detection task zero is as terrible 100
64:39
is perfect and starting and like in
64:41
around 2011 the best state of the art at
64:44
pre deep learning was getting something
64:46
like 5 on this task and now when we use
64:50
an object detection method with Aleks
64:51
that we got 15 and then using the exact
64:54
same object detection method but just
64:56
swapping out looks that for vgg and
64:58
doing everything else the same gave they
64:59
gave us a boost from 15 to 19 and that's
65:02
just the result of using a more powerful
65:03
bigger network that works better an
65:05
image net also gave us improvements on
65:07
this new task and then then if you
65:11
compare this 29 and is 36 again this is
65:14
the exact same object detection method
65:16
one but one is using vgg and the other
65:18
is using a 50 a 50 layer ResNet and
65:21
again the jump from vgg to ResNet gave
65:23
huge gains in performance not just on
65:26
image net but also on a ton of ton of
65:28
downstream tasks and this trend
65:29
basically continued that models that
65:31
work better on image net tended to give
65:33
you gains nearly for free with very
65:35
little effort on a wide range of
65:37
downstream tasks in computer vision so
65:41
then kind of the the the quick guide of
65:43
how to approach transfer learning with
65:44
cnn's
65:45
is i'd like to think about this little
65:46
two-by-two matrix where you can kind of
65:48
think about your problem falling into
65:49
one of these buckets one is whether your
65:52
data set is very similar to image net
65:53
and the other is how much data you have
65:56
in this new data set well if your data
65:59
set is very similar to image net that is
66:01
tends to contain objects that look kind
66:03
of like image net objects then if even
66:06
if you have very very little data then
66:08
like maybe tens to hundreds per category
66:09
then applying some kind of linear
66:11
classifier on top of your pre trained
66:13
features tends to work quite well and if
66:15
you have a fairly large amount of data
66:17
maybe hundreds to thousands of samples
66:19
per category then fine-tuning your an
66:21
image that model on your new data set
66:23
to work quite well now if your dataset
66:26
is fairly different from imagenet and
66:28
similar and different is not well
66:31
defined in this context I have met but
66:33
if your dataset is fairly different from
66:35
imagenet but you have a lot of data
66:37
often you can still initialize from an
66:39
image net model and fine-tune and get
66:41
good performance now the danger zone is
66:44
right here where you have a very small
66:47
data set and the nature of that data is
66:49
somehow very different from the types of
66:51
images you see an image that and if
66:52
that's the case then you're in trouble I
66:54
would I still think that some linear
66:57
classifier from a pre trained model or
66:59
some fine-tuning approach will often
67:01
give you reasonable results but this is
67:03
really the danger zone where you did you
67:04
need to look out for so I'd also like to
67:08
point out that this idea of transfer
67:10
learning in computer vision has really
67:12
become the norm and it has become the
67:13
mainstream way that we operate on many
67:15
tasks in computer vision over the past
67:17
several years that this is really the
67:19
norm not the exception that basically
67:21
most computer very wide majority a very
67:25
wide set of computer vision papers are
67:27
using some kind of transfer learning
67:28
these days so we've seen this already in
67:31
object detection where it is using
67:33
somehow part of the model was pre
67:36
trained on imagenet we'll also see this
67:37
again in things like image captioning
67:39
where again part of the model it was pre
67:41
trained on imagenet and even what's even
67:44
people get even wilder and sort of pre
67:47
trained different parts of the model on
67:49
different data sets and then plug them
67:50
together
67:51
and achieve some end result so for some
67:54
some kind of image captioning model that
67:55
we'll talk about in more detail may be
67:57
part of the model was pre tried on image
67:59
net and then part of the model was pre
68:00
trained on some other data set and then
68:02
you put them together and continue
68:04
training for some eventual downstream
68:05
tasks so somehow I think one important
68:09
shift in computer vision over the past
68:10
several years has actually been finding
68:13
pipelines of pre training and
68:15
fine-tuning and possibly multiple
68:17
multiple rounds of it in order to make
68:19
use of different types of data for your
68:21
eventual ends tasks and a very great
68:24
recent example of this type of approach
68:27
in computer vision actually comes from a
68:29
very recent paper from our very own GSI
68:31
go away where he has this very recent
68:34
paper where they step one train a CNN on
68:37
image now
68:37
step2 fine-tune that CNN for object
68:40
detection on another data set called
68:41
visual genome step 3
68:43
train something called a language model
68:45
called Bert on a different type of data
68:47
set the details of which are not
68:49
important right now step 4 you combine
68:51
the results of two and three and then
68:53
fine-tune them for some kind of a joint
68:55
vision language task and then step five
68:57
you fine-tune this eventual construction
69:00
on your eventual downstream tasks be it
69:02
image captioning or visual
69:04
question-answering or some other kind of
69:06
eventual downstream tasks and I don't
69:08
intend for you to understand the full
69:10
details of what's going on in this model
69:11
this is just to give you a sense that
69:13
these this procedure of fine-tuning and
69:16
pre training and fine-tuning and
69:18
transfer learning has become a very
69:20
critical part of mainstream computer
69:21
vision research and we'll have a guest
69:24
lecture from blew away later in the
69:25
semester where I think he'll talk in
69:27
more detail about this model in
69:28
particular so even though it transfer
69:32
learning has become very very pervasive
69:34
there's been some very very recent
69:36
results that have somewhat called it
69:37
into question so there's this there's
69:40
this very interesting result from just
69:42
this year that shows that for the task
69:45
of object detection you can actually you
69:47
don't you can actually get away without
69:49
pre-training which is something everyone
69:51
thought was critical for many downstream
69:53
tasks in computer vision the catch is
69:56
that in order to do as well on object
69:58
detection but by training from scratch
70:01
you need to train for three times as
70:02
long so you can do just as well without
70:05
pre-training but it requires you to
70:07
train for many many more iterations and
70:10
the other takeaway from this paper is
70:12
that in situations where you have a very
70:14
little training data then pre training
70:17
also gives you some very good
70:19
performance over fine-tuning so that I
70:21
think this actually meshes with some of
70:23
the earlier intuitions we had whereas if
70:26
you have a very very small data set then
70:28
something like tens of samples per class
70:30
then some then something like pre
70:33
training and fine tuning is very very
70:35
effective and if you have larger data
70:37
sets if your data set size is larger
70:39
then you can consider not only
70:40
fine-tuning the whole model or perhaps
70:42
also training a brand new model from
70:44
scratch but the caveat here is that even
70:48
in cases where you have a
70:50
large data sets I think pre-training and
70:52
fine-tuning is still an extremely
70:54
effective recipe in computer vision
70:55
because it makes things trained a lot
70:57
faster so even in cases where you do
70:59
have a fairly large data set at the end
71:01
of the day for the task you care about
71:03
then initializing your model from a
71:05
model pre trained on imagenet tends to
71:07
make things train much much faster so
71:09
it's very very useful to do in practice
71:12
so I'd like to go I think this one this
71:14
part is less critical so it's if there's
71:17
any sort of questions about transfer
71:18
learning I'd rather take those here
71:24
alright so then I guess if you can bear
71:28
with me for just like two minutes we can
71:29
blast through this stuff because you
71:31
don't have enough GPUs to do this anyway
71:32
but but basically a couple lectures ago
71:36
we talked about this notion of moving
71:37
from single device to entire sort of
71:40
rack scale or data center scale machine
71:41
learning so the question is how do you
71:43
actually do that so one idea is that
71:46
we'll split our model across GPUs maybe
71:49
one thing you could imagine is you
71:50
partition put some of the layers on one
71:51
GPUs some of the layers of other GPUs
71:53
and the players that I'm not on GPU this
71:55
turns out to be a really bad idea
71:56
because your GPUs will spend a lot of
71:59
time waiting that only one in this kind
72:01
of scheme only one GPU will be executing
72:03
at a time so it'll be a very inefficient
72:04
use of your resources another idea that
72:08
you'll see sometimes by the way this is
72:10
called model parallelism because the
72:12
idea is that you're splitting up your
72:13
model to run different parts of your
72:14
model on different devices
72:15
another thing that's another flavor of
72:18
model parallelism that you'll see is to
72:20
use different parallel brant use the
72:22
split up your model into multiple
72:24
parallel branches and then run those
72:26
different parallel branches on different
72:27
GPU devices and this is the type of
72:29
mechanism that was actually used by yeah
72:32
the original Alex that paper because if
72:33
you recall the poor guy only had GPUs
72:35
with only three gigabytes of memory so
72:37
this was really critical for training
72:39
Alex at that time but this ends up being
72:42
a fairly inefficient way of paralyzing
72:44
across GPUs as well because it requires
72:47
synchronizing it requires a lot of
72:49
communication between GPUs and in
72:51
particular it requires communicating the
72:53
activations and the gradients of the
72:54
loss with respect to those activations
72:56
within the forward and backward passes
72:58
so this tends to be fairly expensive to
73:00
do so instead the way that people
73:03
the way the typical way that people
73:04
instead paralyzed across multiple GPU
73:07
devices is this idea called data
73:08
parallelism so here what we're going to
73:11
do is take your batch of n images and
73:13
then if we're training on to GPU devices
73:15
we'll replicate the model on each of
73:17
those two GPU devices and run and over
73:20
to run a smaller mini mini batch of n
73:22
over two images on each of your
73:24
independent GPUs and now after you split
73:27
if you split across the batch dimension
73:29
now your GPUs can perform most of their
73:31
processing completely independently and
73:32
there's much there's much less need for
73:34
communication across the GPUs so now
73:37
these two GPUs can run forward pass
73:38
completely independently compute loss
73:40
completely independently compute
73:42
gradient with respect to the parameters
73:43
again without any communication between
73:45
GPUs and the only point where you need
73:47
to communicate is at the end of each
73:49
forward and backward pass where these
73:51
GPUs now need to communicate the
73:53
gradient of the loss with respect to the
73:54
parameters and sum those gradients
73:56
across all the GPU devices in order to
73:58
make a gradient step so this is the way
74:01
that people typically take you make use
74:04
of multiple GPU devices these days and
74:08
this this approach actually scales very
74:09
well to very very large numbers of GPUs
74:12
so you can imagine splitting this not
74:14
just across two GPUs in your desktop but
74:16
across eight GPUs on a big server or
74:19
maybe across hundreds of GPUs in a data
74:21
center and again the idea is the same
74:23
that will take our batch of data split
74:26
it evenly across the GPUs do independent
74:28
forward and backward passes on each
74:29
device and the only point of
74:31
communication is where we need to sum
74:33
the gradients across the different
74:35
elements or the different devices
74:37
another variant you'll somewhat
74:39
sometimes see is where you may be
74:41
combined model parallelism within each
74:43
server and then do data parallelism
74:45
across different servers in a data
74:47
center you can imagine this requires
74:48
having a fairly large number of GPUs to
74:50
play with but this this whole prop but
74:53
then basically this this idea of
74:55
training splitting you're utilizing
74:57
multiple devices by splitting across the
74:59
batch dimension works really well but it
75:02
requires you to be able to train with
75:03
very large mini batches so that so then
75:06
basically the goal here is suppose
75:08
you've got a model that works really
75:10
well on a single GPU and now we want to
75:12
scale up to train that same model on a
75:15
very very large
75:16
gpus and the goal here is usually that
75:19
we want to reduce to reduce the overall
75:20
training time of that model so rather
75:23
than training for a very long time on
75:25
one GPU instead we'd like to train for a
75:27
very short amount of time on that larger
75:29
set of GPUs so then hopefully the total
75:32
number of epochs the total number of
75:33
times that we see each element in the
75:35
training set should be the same but we
75:37
will instead form larger mini batches
75:39
and make fewer overall gradient descent
75:41
steps and basically the way the one
75:45
really important trick to do this ends
75:47
up being fairly straightforward and it
75:49
turns out that a very simple heuristic
75:51
that works really well for this large
75:53
batch training is to scale the learning
75:55
rate in the nearly so suppose that
75:57
you've got a model that trains really
75:58
well on one GPU using a learning rate
76:00
alpha and a batch size of n then we can
76:02
we you can usually train on K GPUs with
76:05
a batch size of KN and per GPU and a
76:09
learning rate of K alpha so basically
76:11
you scale the number of devices by K you
76:13
scale the batch size by K you also scale
76:15
the learning rate by K and this is the
76:17
most important trick for getting things
76:18
to work in very with very large batches
76:20
the other trick is that when if you
76:24
imagine our batches are very very large
76:26
and we're maybe trading up that 1,000
76:28
GPUs then that learning rate is going to
76:30
be very very large because now we're
76:32
training like 1000 times whatever our
76:34
old learning rate was and then the
76:36
problem is that this very large learning
76:37
rate can sometimes explode in the very
76:39
first iteration of training so then what
76:41
people do is they often use a learning
76:43
rate warm-up sched well will where
76:45
they'll start the learning rate from
76:46
zero and then gradually increase the
76:48
learning rates over the first maybe
76:50
thousand or five thousand iterations of
76:51
training or so and then at that point
76:53
they'll continue with whatever other
76:55
learning rate schedule they would have
76:57
used originally there's a couple other
77:00
tricks that you need to use to get this
77:02
to actually work and I would recommend
77:04
you check out this paper if you're
77:05
interested in them but the basic idea is
77:07
that once you can once you get all these
77:09
tricks to work people were able to train
77:11
models on imagenet very very quickly
77:14
using a very very large number of GPUs
77:16
so here this this relatively well-known
77:18
paper was able to train models on
77:20
imagenet in just one hour but the trick
77:23
though that the secret is that they use
77:24
the batch size of 8,192 and they
77:27
distributed this across 256 GPUs
77:30
and they had kind of a click bTW title
77:32
of the paper called imagenet in one hour
77:34
and the problem is that once you write a
77:38
paper are called imagenet in one hour
77:40
you are just begging for people to try
77:42
to come and beat you so absolutely
77:45
so after this paper came out it seemed
77:47
like all the big industrial labs were
77:49
just tripping over themselves to try to
77:51
compete and see how many hardware
77:53
resources could they throw at this
77:54
problem and how fast could they train on
77:55
imagenet yeah question yeah the question
77:58
is um will you asymptotically reach a
77:59
limit as your hardware increases I think
78:02
in theory yes but we haven't hit the
78:04
asymptotes yet so let's keep going so
78:07
after this paper came out they got one
78:09
hour of training on 256 GPUs so not long
78:13
after another paper train with a batch
78:15
size of 12,000 it was a group from
78:17
Intel's they were pushing Intel's new
78:19
Knights landing devices that was kind of
78:21
like a GPU and they got at the tree in
78:23
39 minutes does a great huge improvement
78:25
then another group at Intel came out
78:28
they trained with a batch size of 16,000
78:31
and they trained it on 1,600 Xeon CPUs
78:34
and they got off to train in 31 minutes
78:36
and then another group came out and they
78:39
got it to train in 15 minutes
78:41
but they trained with a batch size of
78:43
32,000 on a thousand GPUs and if you
78:46
look this is like kind of linear right
78:48
if you compare this original image net
78:51
in one hour then you know you can train
78:53
four times as fast with four times as
78:54
big a batch size on four times as many
78:56
devices so in practice if you read
78:58
papers from big industrial labs they'll
79:00
often use these tricks and practice

?? (?? ???)


